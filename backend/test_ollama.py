#!/usr/bin/env python3
"""
Test Ollama Integration
Comprehensive test for local embedding generation using Ollama
"""

import requests
import asyncio
import json

def test_ollama_service():
    """Test if Ollama service is running"""
    print("ğŸ” Testing Ollama Service...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [model.get("name", "").split(":")[0] for model in models]
            print(f"âœ… Ollama is running. Available models: {model_names}")
            return True, model_names
        else:
            print(f"âŒ Ollama responded with status {response.status_code}")
            return False, []
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to Ollama. Is it running?")
        print("ğŸ’¡ Start Ollama with: ollama serve")
        return False, []
    except Exception as e:
        print(f"âŒ Error testing Ollama: {e}")
        return False, []

def test_embedding_model():
    """Test embedding generation with Ollama"""
    print("\nğŸ§  Testing Embedding Generation...")
    
    test_text = "This is a test sentence for embedding generation."
    
    try:
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json={
                "model": "nomic-embed-text",
                "prompt": test_text
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            embedding = result.get("embedding", [])
            
            if embedding:
                print(f"âœ… Embedding generated successfully!")
                print(f"ğŸ“ Dimension: {len(embedding)}")
                print(f"ğŸ“Š Sample values: {embedding[:5]}...")
                return True, len(embedding)
            else:
                print("âŒ No embedding in response")
                return False, 0
        else:
            print(f"âŒ Ollama API error: {response.status_code}")
            print(f"Response: {response.text}")
            return False, 0
            
    except Exception as e:
        print(f"âŒ Error generating embedding: {e}")
        return False, 0

async def test_embedding_service():
    """Test the EmbeddingService with Ollama"""
    print("\nğŸ”§ Testing EmbeddingService Integration...")
    
    try:
        from services.embeddings import EmbeddingService
        
        # Initialize service
        service = EmbeddingService()
        print(f"âœ… EmbeddingService initialized with provider: {service.provider}")
        
        # Test embedding generation
        test_texts = [
            "Machine learning is a subset of artificial intelligence.",
            "Deep learning uses neural networks with multiple layers.",
            "Natural language processing helps computers understand text."
        ]
        
        embeddings = await service.generate_embeddings(test_texts)
        
        if embeddings and len(embeddings) == len(test_texts):
            print(f"âœ… Generated {len(embeddings)} embeddings")
            print(f"ğŸ“ Embedding dimension: {len(embeddings[0])}")
            
            # Test storage (will use Pinecone if available, test mode otherwise)
            test_pdf_id = "test_ollama_pdf"
            storage_ids = await service.store_embeddings(test_texts, test_pdf_id)
            print(f"âœ… Stored embeddings with IDs: {storage_ids}")
            
            # Test similarity search
            search_results = await service.similarity_search("machine learning", test_pdf_id, top_k=2)
            print(f"âœ… Similarity search returned {len(search_results)} results")
            
            return True
        else:
            print("âŒ Embedding generation failed")
            return False
            
    except Exception as e:
        print(f"âŒ EmbeddingService test failed: {e}")
        return False

def test_model_pull():
    """Test pulling the embedding model if not available"""
    print("\nğŸ“¥ Checking/Pulling Embedding Model...")
    
    # Check if model is available
    ollama_running, models = test_ollama_service()
    if not ollama_running:
        return False
    
    if "nomic-embed-text" not in models:
        print("ğŸ“¥ nomic-embed-text model not found. Attempting to pull...")
        print("ğŸ’¡ This may take a few minutes for the first time...")
        
        try:
            # Note: This is a simplified check. In practice, you'd use subprocess
            # to run `ollama pull nomic-embed-text`
            print("ğŸ”§ Please run manually: ollama pull nomic-embed-text")
            return False
        except Exception as e:
            print(f"âŒ Error pulling model: {e}")
            return False
    else:
        print("âœ… nomic-embed-text model is available")
        return True

async def main():
    """Run all Ollama tests"""
    print("ğŸ§ª Ollama Integration Testing")
    print("=" * 50)
    
    results = {}
    
    # Test 1: Ollama service
    ollama_running, models = test_ollama_service()
    results["ollama_service"] = ollama_running
    
    if not ollama_running:
        print("\nğŸ’¡ SETUP REQUIRED:")
        print("1. Install Ollama: curl -fsSL https://ollama.com/install.sh | sh")
        print("2. Start Ollama: ollama serve")
        print("3. Pull model: ollama pull nomic-embed-text")
        return
    
    # Test 2: Model availability
    model_available = test_model_pull()
    results["model_available"] = model_available
    
    if not model_available:
        print("\nğŸ’¡ MODEL SETUP REQUIRED:")
        print("Run: ollama pull nomic-embed-text")
        return
    
    # Test 3: Direct embedding test
    embedding_works, dimension = test_embedding_model()
    results["embedding_generation"] = embedding_works
    
    # Test 4: Service integration
    service_works = await test_embedding_service()
    results["service_integration"] = service_works
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 50)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name.replace('_', ' ').title()}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\nğŸ‰ SUCCESS: Ollama is fully working!")
        print("ğŸ’¡ Your system now has unlimited, free embeddings!")
        print("\nğŸš€ Next steps:")
        print("   - Set EMBEDDING_PROVIDER=ollama in .env")
        print("   - Restart your server: python main.py")
        print("   - Test your PDF quiz system!")
    else:
        print("\nâš ï¸  ISSUES DETECTED")
        print("ğŸ’¡ Follow the setup instructions above")
        print("ğŸ“– See OLLAMA_SETUP.md for detailed guide")

if __name__ == "__main__":
    asyncio.run(main())